{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3EU9nI8EWbrS"
      },
      "source": [
        "## **Assignment One (EECS3404 W25)**\n",
        "\n",
        "**your name:**  arianne ghislaine rull \\\\\n",
        "**your student number:**  219129527 \\\\\n",
        "**your yorku email:** arull@yorku.com\n",
        "\n",
        "This assignment is mainly for you to review mathematical background and vectorization-based programming. You have to work individually. Remember to fill in your information (name, student number, email) at above.\n",
        "\n",
        "\n",
        "\n",
        "##**What to Submit**\n",
        "\n",
        "Please use this notebook to complete assignment one. You have to run your codes and show the results in this notebook. Download the completed notenook as `.ipynb` and compress it as a `.zip` file to submit to eClass.  Submit only ONE notebook file that contains all of your answers and codes to eClass before the deadline.  No late submission\n",
        "will be accepted.\n",
        "\n",
        "* For all written parts, write your answers in text cells. To avoid confusions in marking, better to embed latex codes there to represent all mathematical notations and equations.  **No handwriting is accepted**.\n",
        "\n",
        "* For programming parts, you should give codes, comments, explanations and the proper running outputs in both code and text cells. Make your jupyter notebook clean and concise. Remove all unused codes and all intermediate results from the submitted notebook. The submitted notebook should include only the final (best) outputs for each question. Also make sure every code cell runnable so that markers can reproduce the outputs if necessary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1OwmDh6Zv_b"
      },
      "source": [
        "### **Q1** (10 marks)  **Matrix Multiplication** ###\n",
        "\n",
        "Given two sets of $m$ vectors, $\\mathbf{x}_i \\in \\mathbb{R}^n$  and $\\mathbf{y}_i \\in \\mathbb{R}^n$ for all $i=1,2, \\cdots, m$, verify that the summation $\\sum_{i=1}^m  \\mathbf{x}_i  \\mathbf{y}_i^\\intercal$ can be vectorized as the following matrix multiplication:\n",
        "$$\n",
        "\\sum_{i=1}^m \\mathbf{x}_i \\mathbf{y}_i^\\intercal = \\mathbf{X}  \\mathbf{Y}^\\intercal,\n",
        "$$\n",
        "where $\\mathbf{X} = \\big[ \\mathbf{x}_1 \\, \\mathbf{x}_2 \\, \\cdots \\, \\mathbf{x}_m \\big] \\in \\mathbb{R}^{n\\times m}$ and $\\mathbf{Y} = \\big[ \\mathbf{y}_1 \\, \\mathbf{y}_2 \\, \\cdots \\, \\mathbf{y}_m \\big] \\in \\mathbb{R}^{n\\times m}$."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "wkpFlnUhvwew"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIWKLT7taeil"
      },
      "source": [
        "*write your answer to Q1 here:*\n",
        "\n",
        "\n",
        "### Definitions\n",
        "\n",
        "1. Define $ \\mathbf{x}_i \\in \\mathbb{R}^n  $ as a column vector of dimension $  n  $.\n",
        "2. Define $ \\mathbf{y}_i \\in \\mathbb{R}^n \\ $ as another column vector of dimension $ n $.\n",
        "3. Let $ \\mathbf{X} = [\\mathbf{x}_1 \\, \\mathbf{x}_2 \\, \\cdots \\, \\mathbf{x}_m] \\in \\mathbb{R}^{n \\times m} \\ $ be a matrix where each column corresponds to  $  \\mathbf{x}_i $.\n",
        "4. Similarly, let $ \\ \\mathbf{Y} = [\\mathbf{y}_1 \\, \\mathbf{y}_2 \\, \\cdots \\, \\mathbf{y}_m] \\in \\mathbb{R}^{n \\times m} \\ $.\n",
        "\n",
        "### Proof\n",
        "\n",
        "1. **Matrix Multiplication**: By definition, $ \\mathbf{Y}^T \\in \\mathbb{R}^{m \\times n} \\ $ is the transpose of  $ i$-th row of $ ( \\mathbf{Y}^T ) $ corresponds to the $ i$-th column of $ \\mathbf{Y} \\ $\n",
        "2. Consider the product $ ( \\mathbf{X} \\mathbf{Y}^T) $. The result is a matrix of size $ ( n \\times n ) $. Each element of $ ( \\mathbf{X} \\mathbf{Y}^T) $ can be expressed as:\n",
        "\n",
        "$$\n",
        "\\left( X Y^T \\right)_{jk} = \\sum_{i=1}^m \\left( \\mathbf{x}_i \\right)_j \\left( \\mathbf{y}_i \\right)_k\n",
        "$$\n",
        "\n",
        "   where $ \\ (\\mathbf{x}_i)_j \\ $ is the $ j $-th element of the $ i $-th vector $  \\mathbf{x}_i \\ $, and $ (\\mathbf{y}_i)_k \\ $ is the $ k $-th element of the $ i$-th vector $ \\mathbf{y}_i  $\n",
        "\n",
        "3. **Vectorized Summation**: Now consider the summation $ \\ \\sum_{i=1}^m \\mathbf{x}_i \\mathbf{y}_i^T \\ $. Each term in the summation is an outer product of $  \\mathbf{x}_i \\in \\mathbb{R}^n \\ $ and $ \\mathbf{y}_i^T \\in \\mathbb{R}^n  $ , resulting in an $  n \\times n  $ matrix. The $  j, k  $ -th element of this sum is:\n",
        "$\n",
        "   [\n",
        "   \\left( \\sum_{i=1}^m \\mathbf{x}_i \\mathbf{y}_i^T \\right)_{jk} = \\sum_{i=1}^m (\\mathbf{x}_i)_j (\\mathbf{y}_i)_k\n",
        "   ]\n",
        "$\n",
        "\n",
        "4. Comparing with the definition of $  (\\mathbf{X}\\mathbf{Y}^T)_{jk}  $, we see that:\n",
        "$\n",
        "   [\n",
        "   \\sum_{i=1}^m \\mathbf{x}_i \\mathbf{y}_i^T = \\mathbf{X}\\mathbf{Y}^T\n",
        "   ]\n",
        "$\n",
        "\n",
        "Thus, $ \\sum_{i=1}^m \\mathbf{x}_i \\mathbf{y}_i^T = \\mathbf{X}\\mathbf{Y}^T $\n",
        "\n",
        "We confirmed that the left hand and right hand side are equal.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "FQo3R1A707CD"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgoTthU7aoV2"
      },
      "source": [
        "### **Q2** (20 marks)  **Vectorized Programming** ###\n",
        "\n",
        "**Part 2.1 (10 marks):**\n",
        "\n",
        "Given a set of $N$ feature vectors,\n",
        "$$\n",
        "\\big\\{ \\mathbf{x}_1, \\mathbf{x}_2, \\cdots, \\mathbf{x}_N  \\big\\}\n",
        "$$\n",
        "where each vector $\\mathbf{x}_i \\in \\mathbb{R}^d$,\n",
        "write a python program to calculate the pairwise inner products and store the results in a matrix ${\\mathbf A} \\in \\mathbb{R}^{N \\times N}$: the element located\n",
        "at the $i$-th row and $j$-th column represents the inner product as follows:\n",
        "$$\n",
        "a_{ij} =  \\mathbf{x}_i \\cdot \\mathbf{x}_j  \\;\\;\\;\\;\\; (1 \\leq i,j \\leq N)\n",
        "$$\n",
        "\n",
        "Discuss why the following code is NOT an efficient implementation. Next, rewrite a vectorized program for better implementation efficiency. Compare your code with the provided one in terms of running speed. Also provide a detailed mathematical derivation to explain your vectorization-based implementation."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "N = 5000 # generate N=5000 samples\n",
        "d = 784  # dimension of each vector\n",
        "\n",
        "# generate all samples and save them in matrix X, where each row represents a vector x_i\n",
        "X = np.random.normal(size=(N,d))\n",
        "\n",
        "def inner_product_loops(X):\n",
        "  N = X.shape[0]\n",
        "  A = np.zeros((N,N))\n",
        "  for i in range(N):\n",
        "    for j in range(N):\n",
        "      A[i,j] = np.dot(X[i,:], X[j,:])\n",
        "  return A\n",
        "\n",
        "%timeit inner_product_loops(X)"
      ],
      "metadata": {
        "id": "mCzwYZANVd1d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kId5VtjBgTAu"
      },
      "source": [
        "*write your answer to Q2.1 here:*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why is the code not an efficient implementation?**\n",
        "\n",
        "1. Time Complexity\n",
        "\n",
        "*   The given code uses nested loops to compute the dot product for every pair of vectors.\n",
        "This computes the inner product of each pair of vectors. The outer loop runs for `N` iterations, and for each iteration, the inner loop also runs for `N` iterations. This time complexity is `O(N^2)` which can get very slow when the n is large.\n",
        "\n",
        "2. Memory Access\n",
        "\n",
        "* This program computes the product using `np.dot(X[i], X[j])`, which requires iterating over all d elements of the vectors each time. The method involves unnecessary recomputations and can be optimized by vectorizing the inner product calculation.\n",
        "\n"
      ],
      "metadata": {
        "id": "-znFominxn3o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Rewrite the code.**\n",
        "\n",
        "\n",
        "Instead of manually computing the dot products using loops, we can use matrix multiplication.\n",
        "\n",
        "The inner product between all pairs of vectors can be represented as the matrix product of the matrix `X` with its transpose.\n",
        "\n",
        "The pairwise dot products can be computed as:\n",
        "\n",
        "$$A = X \\cdot X^T$$\n",
        "\n",
        "\n",
        "\n",
        "*   $ \\mathbf{X} \\$ is the matrix containing all the feature vectors as rows\n",
        "\n",
        "\n",
        "*   $ \\mathbf{X}^T \\ $ is the transpose of $ \\mathbf{X} \\ $\n",
        "\n",
        "* The result of $X \\cdot X^T$ is an $N \\times N$ matrix, where the entry at position $(i, j)$ is the dot product of $\\mathbf{x}_i$ and $\\mathbf{x}_j$.\n"
      ],
      "metadata": {
        "id": "UHYA8-L8xoU4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KmMGLmvwLQWM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b7de403-b576-42f1-f67d-3a7f558d03b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.92 s ± 320 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "# Write your vectorized code for Q2.1 here\n",
        "import numpy as np\n",
        "\n",
        "N = 5000 # generate N=5000 samples\n",
        "d = 784  # dimension of each vector\n",
        "\n",
        "# Generate all samples and save them in matrix X, where each row represents a vector x_i\n",
        "X = np.random.normal(size=(N, d))\n",
        "\n",
        "def inner_product_vectorized(X):\n",
        "  return np.dot(X, X.T) # Matrix multiplication betweent X and X^T (its transpose)\n",
        "\n",
        "\n",
        "%timeit inner_product_vectorized(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Compare the code.**\n",
        "\n",
        "1. Loop based implementation - this takes much longer to compute due to the loops and repeated row accesses\n",
        "2. Vectorized implementation - this uses efficient matrix multiplication because there is no python loop overhead\n",
        "\n",
        "Based on the measure time taken by the vectorized implementation, it is faster"
      ],
      "metadata": {
        "id": "C05pQiLfnN8J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Add the mathematical derivation.**\n",
        "\n",
        "1. Input is $X$\n",
        "2. Goal is to compute a matrix $A$ of size $ N \\times N $ where each element $A[i,j]$ is the inner product of the vectors\n",
        "3. Instead of calculating $A[i,j]$ one-by-one loops, we use matrix multiplication:\n",
        "$$A = X \\cdot X^T$$\n",
        "\n",
        "Here:\n",
        "* $X$ is the original matrix ($N \\times d$)\n",
        "* $X^T$ is the transpose of $X$ ($N \\times d$)\n",
        "\n",
        "When we multiply $X$ and $X^T$ the result is an $N \\times N$ matrix A where:\n",
        "\n",
        "$$A[i,j] = \\mathbf{x}_i \\cdot \\mathbf{x}_j$$\n",
        "\n"
      ],
      "metadata": {
        "id": "khxNYzKnodMW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHbGYAfSL8Q2"
      },
      "source": [
        "**Part 2.2 (10 marks):**\n",
        "\n",
        "Assume that we want to compute the Euclidean distance between another vector $\\mathbf{y} \\in \\mathbb{R}^d$ and every vector $\\mathbf{x}_i $ in the above set. Write a vectorized program to compute these distances and save them in a vector. Note that you need to provide a detailed mathematical derivation to explain your vectorization-based implementation."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. write a vectorized program to compute and save them in a vector\n",
        "2. mathematical derivation"
      ],
      "metadata": {
        "id": "2umB_l2u5yZE"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wweW8AMlONz-"
      },
      "source": [
        "*write your answer to Q2.2 here:*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Euclidean Distance**\n",
        "\n",
        "the euclidean distance between two vectors **x<sub>i</sub>** and **y** is given by:\n",
        "\n",
        "![Screenshot 2025-01-22 at 07.45.50.png](data:image/png;base64,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)\n"
      ],
      "metadata": {
        "id": "fcU773JjAf1V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This means:\n",
        "1. Subtract each element of $y$ from the corresponding element of **x<sub>i</sub>**\n",
        "2. Square the result of each subtraction.\n",
        "3. Add up all these squares.\n",
        "4. Finally, take the square root of this sum."
      ],
      "metadata": {
        "id": "BL6p_DOYDI8y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What if we want to do this for all vectors?\n",
        "\n",
        "We have a set of N vectors **{x<sub>1</sub>, x<sub>2</sub>,...,x<sub>n</sub>}** and want to compute the distance between **y** and each **x<sub>i</sub>**\n",
        "\n",
        "To make this efficient, we can do it all for $N$ vectors at once by using matrix operations."
      ],
      "metadata": {
        "id": "-pbBXN7WHRWe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step-by-Step Derivation for All Vectors:\n",
        "\n",
        "1. Subtraction\n",
        "\n",
        "\n",
        "*   For each $ \\mathbf{x}_i $, subtract $y$ element-wise\n",
        "\n",
        "* If $X$ is a matrix where each row is a vector $ \\mathbf{x}_i $ , then $ X - y $ subtracts $y$ from each row of $X$\n",
        "\n",
        "*   Result: A new matrix of size $ N \\times d$, where each row is $ \\mathbf{x}_i - \\mathbf{y} $\n",
        "\n",
        "\n",
        "\n",
        "2. Square the differences\n",
        "*   Square each element of the result from step 1 to get $  (x_{i,k} - y_k)^2  $\n",
        "* Result: Another $N \\times d$ matrix, where each row contains the square differences for a vector $ $\n",
        "\n",
        "3. Sum the squares\n",
        "* Add up all the squared differences for each vector $ x_{i}$\n",
        "* Mathematically: $ \\sum_{k=1}^{d} (x_{i,k} - y_k)^2 $\n",
        "* Result: A vector of size $N$, where each element is the sum of squared differences for a specific $x_{i}$\n",
        "\n",
        "4. Take the square root\n",
        "* Take the square root of each element in the result from step 3 to get the final Euclidean distances.\n",
        "* Result: A vector of size $N$, where each element is the distance $d(x_{i}, y)$"
      ],
      "metadata": {
        "id": "rkS8a5bYNHph"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Final Formula in Matrix Form:\n",
        "For all $N$ vectors in $X$, the Euclidean distances can be computed as:\n",
        "\n",
        "$ \\mathbf{d} = \\sqrt{\\text{row-wise sum of } (\\mathbf{X} - \\mathbf{y})^2} $\n",
        "\n"
      ],
      "metadata": {
        "id": "Ic1PINXwNOKf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQWgljIaONMe"
      },
      "outputs": [],
      "source": [
        "# Write your vectorized code for Q2.2 here"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Generate N vectors (X) of dimension d and a single vector y\n",
        "N = 5000 # number of vectors\n",
        "d = 784  # dimension of each vector\n",
        "X = np.random.normal(size=(N, d)) # Matrix of N vectors\n",
        "y = np.random.normal(size=(d,)) # Single vector\n",
        "\n",
        "def euclidean_distance_vectorized(X, y):\n",
        "  # Compute the squared differences\n",
        "  squared_diff = (X - y) ** 2\n",
        "  # Sum along the second axis (dimension d)\n",
        "  sum_squared_diff = np.sum(squared_diff, axis=1)\n",
        "  # Take the square root\n",
        "  # take the square root in order to get the euclidean distances\n",
        "  distances = np.sqrt(sum_squared_diff)\n",
        "  return distances\n",
        "\n",
        "# compute the distances\n",
        "distances = euclidean_distance_vectorized(X, y)\n",
        "print(distances)\n"
      ],
      "metadata": {
        "id": "rwI3lem6HQyS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63529026-c801-4f3c-a824-14784133482e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[40.28945771 39.22085223 40.3527181  ... 39.08675243 39.36479703\n",
            " 40.9373935 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code Explanation\n",
        "1. Subtraction:\n",
        "- $X - y$ computes the difference between each row of $X$ (a vector $x_{i}$) and y. Broadcasting ensures $y$ is subtracted from all rows of $X$\n",
        "2. Element-wise Square:\n",
        "- $(X-y)$ ** 2 squares each element of the resulting matrix\n",
        "3. Row-wise Summation:\n",
        "- np.sum(squared_diff, axis=1) sums the squared differences along each row (dimension d), yielding a vector of size $N$\n",
        "4. Square Root:\n",
        "- np.sqrt(sum_squared_diff) computes the square root of each element to get the final Euclidean distances\n",
        "\n",
        "\\\n",
        "\\begin{array}{|c|c|}\n",
        "\\hline\n",
        "\\textbf{Operation} & \\textbf{Time Complexity} \\\\ \\hline\n",
        "\\text{Subtraction} & O(N \\cdot d) \\\\ \\hline\n",
        "\\text{Element-wise Square} & O(N \\cdot d) \\\\ \\hline\n",
        "\\text{Summation} & O(N \\cdot d) \\\\ \\hline\n",
        "\\text{Square Root} & O(N) \\\\ \\hline\n",
        "\\textbf{Overall} & O(N \\cdot d) \\\\ \\hline\n",
        "\\end{array}\n",
        "\n",
        "\n",
        "\n",
        "Why is this efficient?\n",
        "Because there is no loops and it uses Numpy's optimized backend which results into a faster execution\n",
        "\n",
        "The loop based approach would compute the distance for each vector $x_{i}$ individually with a time complexity of $ O(N \\cdot d) $ but with higher constant factors due to the Python loop overhead\n",
        "\n"
      ],
      "metadata": {
        "id": "kDL8KYzPNSRK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KabrtvbSOx6P"
      },
      "source": [
        "### **Q3** (20 marks) **k-NN** ###\n",
        "\n",
        "**Part 3.1 (10 marks):**\n",
        "Study [the given sample code](https://colab.research.google.com/drive/1JS3l6116ldySnReIWhP5wQxKVSehm7cj), and then modify the code to apply the $k$-NN method to classify the iris data using all provided four features. Show the classification accuracies for different $k$ values.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ji-9h0i4P8NI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e362f7d5-dcee-43d6-d25e-6d273e64b7bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(112, 4)\n",
            "(38, 4)\n",
            "(112,)\n",
            "(38,)\n"
          ]
        }
      ],
      "source": [
        "# Write your code for Q3.1 here\n",
        "\n",
        "# the algorithm\n",
        "\n",
        "# step 1: import the modules\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# step 2: load iris dataset\n",
        "iris = load_iris()\n",
        "\n",
        "# step 3: use all four features for the dataset\n",
        "\n",
        "# load all the features\n",
        "\n",
        "X = iris.data\n",
        "\n",
        "# target the labels\n",
        "y = iris.target\n",
        "\n",
        "\n",
        "# split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=7)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# seeing the accuracy results\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "\n",
        "for k in range(1, 11):\n",
        "  # build the kNN model\n",
        "  classifier  = KNeighborsClassifier(n_neighbors=k)\n",
        "  # train the kNN\n",
        "  classifier.fit(X_train, y_train)\n",
        "  # predict on test set\n",
        "  y_pred = classifier.predict(X_test)\n",
        "\n",
        "  # get the accuracy score\n",
        "  accuracy = accuracy_score(y_test, y_pred)*100\n",
        "  print(f\"kNN model accuracy (k={k}) is {accuracy:.2f}%\" )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fEwBiw3pOSQ",
        "outputId": "044cc63c-1c14-43f1-8449-22c0bfb730d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kNN model accuracy (k=1) is 97.37%\n",
            "kNN model accuracy (k=2) is 97.37%\n",
            "kNN model accuracy (k=3) is 97.37%\n",
            "kNN model accuracy (k=4) is 97.37%\n",
            "kNN model accuracy (k=5) is 97.37%\n",
            "kNN model accuracy (k=6) is 97.37%\n",
            "kNN model accuracy (k=7) is 97.37%\n",
            "kNN model accuracy (k=8) is 97.37%\n",
            "kNN model accuracy (k=9) is 97.37%\n",
            "kNN model accuracy (k=10) is 97.37%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwFR0faQQUgG"
      },
      "source": [
        "**Part 3.2 (10 marks):**\n",
        "Based on your vectorized program in Q2.2, instead of using `sciki-learn`, implement the $k$-NN method from scratch (only for Euclidean distance and uniform voting). Apply your method to the iris data as in Q3.1 and compare your code with that of Q3.1 in terms of accuracy and running speed. Discuss your findings."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. apply\n",
        "2. compare code\n",
        "3. discuss finidngs"
      ],
      "metadata": {
        "id": "Xpq8AOdOvUmd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1. Load and Split the Iris Data###\n",
        "\n"
      ],
      "metadata": {
        "id": "RIm8GruJmMFg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4c12IClfhZiS"
      },
      "outputs": [],
      "source": [
        "# Write your code for Q3.2 here\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import time\n",
        "\n",
        "# then load the iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # shape: (150, 4)\n",
        "y = iris.target # shape: (150, )\n",
        "\n",
        "# split into train and test sets for evaluatiobn\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=7)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2. Vectorized Euclidean Distance Function###\n",
        "\n",
        "This function calculates the Euclidean distance between all rows in the test set $(X_{test})$ and the training set $(X_{train})$ in one go for speed. The output is a matrix where each entry represents the distance between a test sample and a training sample.\n",
        "\n",
        "Steps\n",
        "1. Broadcasting Trick:\n",
        "\n",
        "\n",
        "*   Expand dimensions to align $(X_{test})$ and $(X_{train})$ for subtraction\n",
        "*   Subtract $X_{train}$ from $X_{test}$ resulting in a difference matrix\n",
        "\n",
        "\n",
        "2. Compute Squared Distances:\n",
        "*   Square the differences and sum them along the feature dimension.\n",
        "\n",
        "3. Square Root:\n",
        "*   Take the square root to get the final Euclidean distances\n",
        "\n",
        "The result is a distance matrix of shape (num_test, num_train)\n"
      ],
      "metadata": {
        "id": "wSGVOcTFbIgw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def euclidean_distance_matrix(X_test, X_train):\n",
        "  \"\"\"\n",
        "  Compute the euclidean distance between each row of X_test and each row of X_train.\n",
        "\n",
        "  X_test: (num_test, d)\n",
        "  X_train: (num_train, d)\n",
        "\n",
        "\n",
        "  Returns:\n",
        "  distances: (num_test, num_train)\n",
        "\n",
        "\n",
        "  Broadcasting trick:\n",
        "  X_test[:, None, :] => (num_test, 1, d)\n",
        "  X_train[None, :, :] => (1, num_train, d)\n",
        "  Subtract => (num_test, num_train, d)\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  diff = X_test[:, None, :] - X_train[None, :, :]\n",
        "\n",
        "  # Square and sum along the feature dimension (axis=2)\n",
        "  dist_squared = np.sum(diff**2, axis=2) # shape: (num_test, num_train)\n",
        "\n",
        "  # Square root gives the Euclidean distance\n",
        "  distances = np.sqrt(dist_squared)\n",
        "\n",
        "\n",
        "  return distances\n"
      ],
      "metadata": {
        "id": "uc5CAssyeRWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3. Implement k-NN Classifier From Scratch###\n",
        "Basic k-NN with uniform voting\n",
        "\n",
        "\n",
        "1.   Compute distances: For each test sample, find its distances to all training samples.\n",
        "2.   Find the k nearest neighbors: Sort or partially sort these distances to get the indices of the top k.\n",
        "3.   Majority vote: Among those k neighbors, the predicted class is the one that appears the most frequently.\n",
        "\n"
      ],
      "metadata": {
        "id": "DpmQYvmieS2z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def knn_predict(X_train, y_train, X_test, k):\n",
        "  \"\"\"\n",
        "  Predict the class labels for the X_test using a k-NN classifier with Euclidean distance and uniform voting\n",
        "\n",
        "  Parameters\n",
        "  X_train: (num_train, d)\n",
        "  y_train: (num_train, )\n",
        "  X_test: (num_test, d)\n",
        "  k: int\n",
        "\n",
        "  Returns:\n",
        "  y_pred: (num_test,) Predicted labels for each row of X_test\n",
        "\n",
        "  1. Compute all pairwise distances: shape => (num_test, num_train)\n",
        "  2. For each test row, find the k-nearest neighbors\n",
        "  3. Find the majority vote\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  # 1) Compute all pairwise distances: shape => (num_test, num_train)\n",
        "  distances = euclidean_distance_matrix(X_test, X_train)\n",
        "\n",
        "  # 2) For each test row, find the k nearest neighbors\n",
        "  # argsort along axis=1 sorts each row's distances\n",
        "  sorted_indices = np.argsort(distances, axis=1) # (num_test, num_train)\n",
        "  k_closest = sorted_indices[:, :k] # (num_test, k)\n",
        "\n",
        "  # 3) Majority vote\n",
        "  y_pred = []\n",
        "  for i in range(X_test.shape[0]):\n",
        "    # gather the labels of the k nearest neighbors\n",
        "    neighbor_labels = y_train[k_closest[i]]\n",
        "    # find the most frequent labels\n",
        "    values, counts = np.unique(neighbor_labels, return_counts=True)\n",
        "    majority_label = values[np.argmax(counts)]\n",
        "    y_pred.append(majority_label)\n",
        "\n",
        "  return np.array(y_pred)\n",
        "\n",
        "  # return the predicted y value\n"
      ],
      "metadata": {
        "id": "oX89GvBbnvyb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Compare Accuracy and Running Time with scikit-learn###\n",
        "\n",
        "we will evaluate for k from 1 to 10. for each k:\n",
        "\n",
        "\n",
        "*   Our custom k-nn: We will time how long it takes to predict and then compute the accuracy on X_test.\n",
        "\n",
        "*   Scikit-learn kNeighborsClassifier: Fit and predict. Then, we will time it and compare accuracy.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hsAMJclfRqPz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "print(\" k | Custom k-NN (Acc %) | scikit-learn (Acc %) | Custom Time (s) |  Sklearn Time (s)\")\n",
        "print(\"---|---------------------|----------------------|-----------------|------------------\")\n",
        "\n",
        "for k in range (1,11):\n",
        "  #1 custom k-NN\n",
        "  start_custom = time.time()\n",
        "  y_pred_custom = knn_predict(X_train, y_train, X_test, k)\n",
        "  end_custom = time.time()\n",
        "  # custom model we made accuracy\n",
        "  custom_accuracy = accuracy_score(y_test, y_pred_custom) * 100\n",
        "  custom_time = end_custom - start_custom\n",
        "\n",
        "  #2 sci-kit learn k-NN\n",
        "  start_sklearn = time.time()\n",
        "  sk_knn = KNeighborsClassifier(n_neighbors=k)\n",
        "  sk_knn.fit(X_train, y_train)\n",
        "  y_pred_sklearn = sk_knn.predict(X_test)\n",
        "  end_sklearn = time.time()\n",
        "  sklearn_accuracy = accuracy_score(y_test, y_pred_sklearn) * 100\n",
        "  sklearn_time = end_sklearn - start_sklearn\n",
        "\n",
        "  print(f\"{k:2d} |      {custom_accuracy:6.2f}        |       {sklearn_accuracy:6.2f}       \"\n",
        "          f\"|     {custom_time:8.5f}  |     {sklearn_time:8.5f}\")\n"
      ],
      "metadata": {
        "id": "i0CKetLkSPKW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5906e14-d67b-4274-c287-a09096289d72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " k | Custom k-NN (Acc %) | scikit-learn (Acc %) | Custom Time (s) |  Sklearn Time (s)\n",
            "---|---------------------|----------------------|-----------------|------------------\n",
            " 1 |       97.37        |        97.37       |      0.00206  |      0.00629\n",
            " 2 |       97.37        |        97.37       |      0.00153  |      0.00332\n",
            " 3 |       97.37        |        97.37       |      0.00131  |      0.00313\n",
            " 4 |       97.37        |        97.37       |      0.00143  |      0.00335\n",
            " 5 |       97.37        |        97.37       |      0.00186  |      0.00405\n",
            " 6 |       97.37        |        97.37       |      0.00158  |      0.00341\n",
            " 7 |       97.37        |        97.37       |      0.00174  |      0.00420\n",
            " 8 |       97.37        |        97.37       |      0.00136  |      0.00323\n",
            " 9 |       97.37        |        97.37       |      0.00183  |      0.00347\n",
            "10 |       97.37        |        97.37       |      0.00165  |      0.00348\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Now Let's Discuss Our Findings###\n",
        "\n",
        "\n",
        "\n",
        "1.   Accuracy\n",
        "- For the iris dataset, we have high accuracies for both the custom k-NN and scikit-learn's classifier\n",
        "- Fundamentally, they use the same algorithm (Euclidean distance and uniform voting). Hence, the yield relatively identical accuracy\n",
        "\n",
        "2.   Speed\n",
        "- Both implementation run fast but the custom one runs slightly faster than the scikit-learn on a smaller dataset. the iris dataset has around 150 sample size.\n",
        "\n"
      ],
      "metadata": {
        "id": "-4L2B-phUz-a"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jw48ZfUexHP"
      },
      "source": [
        "### **Q4** (20 marks)  **Gradient Descent** ###\n",
        "\n",
        "**Part 4.1 (10 marks):**\n",
        "Given the matrix $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ and the output vector $\\mathbf{y} \\in \\mathbb{R}^m$, in order to solve the following linear equation:\n",
        "$$\n",
        "\\mathbf{A} \\mathbf{x} = \\mathbf{y}\n",
        "$$\n",
        "for the input vector $\\mathbf{x} \\in \\mathbb{R}^n$, we may seek to solve the following optimization problem:\n",
        "$$\n",
        "\\min_{\\mathbf{x}}  \\big\\Vert \\mathbf{A} \\mathbf{x} - \\mathbf{y} \\big\\Vert^2\n",
        "$$\n",
        "\n",
        "Justify why solving this optimization problem is equivallent to solving the original linear equation. Derive a gradient decent algorithm to solve this optimization optimization.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXRhRXQCfLLM"
      },
      "source": [
        "*for Q4.1, describe your gradient descent algorithm here:*\n",
        "\n",
        "###Why Minimizing $ \\big\\Vert \\mathbf{A} \\mathbf{x} - \\mathbf{y} \\big\\Vert^2\n",
        "$ is Like Solving $\\mathbf{A} \\mathbf{x} = \\mathbf{y}$. ###\n",
        "- The optimization problem\n",
        "$$\n",
        "\\min_{\\mathbf{x}}  \\big\\Vert \\mathbf{A} \\mathbf{x} - \\mathbf{y} \\big\\Vert^2\n",
        "$$\n",
        "seeks $x$ that makes $Ax$ as close to $y$ as possible in the Euclidean norm\n",
        "- If there is a solution $x^*$ satisfying $Ax^* = y$ exactly, then $\n",
        " \\big\\Vert \\mathbf{A} \\mathbf{x}^* - \\mathbf{y} \\big\\Vert^2 = 0\n",
        "$ , which is clearly the global minimum of the objective.\n",
        "- Hence, whenever the linear system has a solution, minimizing $\n",
        " \\big\\Vert \\mathbf{A} \\mathbf{x}^* - \\mathbf{y} \\big\\Vert^2 $ is equivalent to finding an $x$ such that $Ax=y$\n",
        "- Even if the system is inconsistent (no exact solution), solving $min_{x} \\big\\Vert \\mathbf{A} \\mathbf{x}^* - \\mathbf{y} \\big\\Vert^2$ gives the least squares solution which is the best approximation in that case"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Deriving the gradient###\n",
        "- Let\n",
        "  $$\n",
        "  f(x) = \\frac{1}{2} \\|Ax - y\\|^2.\n",
        "  $$\n",
        "  We include the factor $\\frac{1}{2}$ for convenience (it simplifies the gradient).\n",
        "\n",
        "- Expanding \\(f(x)\\),\n",
        "  $$\n",
        "  f(x) = \\frac{1}{2} (Ax - y)^\\top (Ax - y).\n",
        "  $$\n",
        "\n",
        "- Taking the gradient w.r.t. \\(x\\):\n",
        "\n",
        "  this is the derivative\n",
        "  $$\n",
        "  \\nabla f(x) = A^\\top (Ax - y).\n",
        "  $$\n",
        "\n",
        "\n",
        "###Iteration of gradient descent###\n",
        "\n",
        "- The basic gradient descent update is\n",
        "  $$\n",
        "  x_{k+1} = x_k - \\eta \\nabla f(x_k),\n",
        "  $$\n",
        "  where $ (\\eta > 0) $is the (constant) step size (a.k.a. learning rate).\n",
        "\n",
        "- Substituting $ (\\nabla f(x_k) = A^\\top (Ax_k - y)) $ , we get\n",
        "  $$\n",
        "  x_{k+1} = x_k - \\eta A^\\top (Ax_k - y).\n",
        "  $$\n",
        "\n",
        "- In practice, $(\\eta)$ must be chosen small enough for convergence. A common choice is\n",
        "  $$\n",
        "  \\eta = \\frac{1}{\\lambda_{\\text{max}}(A^\\top A)},\n",
        "  $$\n",
        "  where $ (\\lambda_{\\text{max}}(A^\\top A)) $ is the largest eigenvalue of $ (A^\\top A) $.\n"
      ],
      "metadata": {
        "id": "kSm71dNrUbJm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary\n",
        "\n",
        "Solving\n",
        "$$\n",
        "\\min_x \\|Ax - y\\|^2\n",
        "$$\n",
        "is equivalent to finding \\(x\\) satisfying \\(Ax = y\\) whenever a solution exists (otherwise it yields the least-squares solution).\n",
        "\n",
        "Defining\n",
        "$$\n",
        "f(x) = \\frac{1}{2} \\|Ax - y\\|^2,\n",
        "$$\n",
        "its gradient is\n",
        "$$\n",
        "\\nabla f(x) = A^\\top (Ax - y).\n",
        "$$\n",
        "\n",
        "The gradient descent iteration is\n",
        "$$\n",
        "x_{k+1} = x_k - \\eta A^\\top (Ax_k - y),\n",
        "$$\n",
        "which converges to the least-squares (or exact) solution for appropriate step size $(\\eta) $.\n"
      ],
      "metadata": {
        "id": "SpLG8D0LWamx"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3mUDnDlfchg"
      },
      "source": [
        "**Part 4.2 (10 marks):**  Use `jax.numpy` and its auto-grad function to write a\n",
        " gradient descent code to solve the above optimization problem for the following two cases. For each case,  print the found solution $\\mathbf{x}^*$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QPl-IOvzemCa",
        "outputId": "ed864472-88dc-4f35-aefe-433ea643e5c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8, 8)\n",
            "(8, 1)\n"
          ]
        }
      ],
      "source": [
        "import jax.numpy as jnp\n",
        "\n",
        "A1 = jnp.array([[ 0.59752613, 0.6905346, -0.50891773, 0.65463308, 0.26701531, -0.56915274, 0.11808333, 0.46735838],\n",
        " [-0.89526606, 0.79715922, 0.57452342, -0.00629485, 0.44091118, -0.90772543, 0.34577912, 0.67014199],\n",
        " [ 0.93670858, 0.72254387, 0.99337376, 0.21567713, 0.05358001, 0.71163904, -0.8129734, -0.77292358],\n",
        " [-0.40845634, 0.74702203, -0.36284625, 0.92230974, 0.98837581, 0.69059759, -0.33978374, 0.59868784],\n",
        " [ 0.09642706, -0.43787392, -0.67600912, -0.45671585, 0.56035694, 0.72904483, 0.79505001, -0.29354031],\n",
        " [-0.59517708, 0.08172389, 0.38847584, 0.21164882, -0.09421962, -0.6612324, -0.7419197, -0.11201114],\n",
        " [-0.90998187, 0.10939955, -0.00644353, -0.50790748, 0.69847656, -0.35544255, -0.78111919, 0.76442594],\n",
        " [ 0.0696891, -0.43123797, 0.87752935, -0.53844923, -0.05382915, -0.77473227, 0.37893365, 0.4033205 ]])\n",
        "\n",
        "y1 = jnp.array([[0.81025244],[0.12633403],[-0.27351843],[-0.22738992],[-0.04246509],[-0.74659567],[0.29532475],[0.29147134]])\n",
        "\n",
        "print(A1.shape)\n",
        "print(y1.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhOzl9nMmGXr",
        "outputId": "95d8af31-da56-41f8-966c-07bf912a36d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10, 6)\n",
            "(10, 1)\n"
          ]
        }
      ],
      "source": [
        "import jax.numpy as jnp\n",
        "\n",
        "A2 = jnp.array([[0.8847363,  0.20735656, 0.3773889,  0.23544965, 0.58455062, 0.60455535],\n",
        " [0.22066618, 0.10340076, 0.82985727, 0.51556355, 0.16158017, 0.01554002],\n",
        " [0.77804031, 0.53846337, 0.52636412, 0.52573696, 0.05653821, 0.9502298 ],\n",
        " [0.28905543, 0.25418091, 0.68436999, 0.36737675, 0.11333675, 0.50065588],\n",
        " [0.0923724,  0.40013578, 0.10427759, 0.88367601, 0.04567698, 0.5614461 ],\n",
        " [0.76325149, 0.05564603, 0.73700669, 0.78701047, 0.3065009,  0.81391347],\n",
        " [0.42297042, 0.06445234, 0.37385898, 0.95497206, 0.98407816, 0.28076653],\n",
        " [0.10806804, 0.76714286, 0.82931698, 0.25355806, 0.09899629, 0.47661276],\n",
        " [0.55615413, 0.32609653, 0.84413152, 0.73315836, 0.58309715, 0.84786528],\n",
        " [0.95206656, 0.1132698,  0.39265378, 0.75970375, 0.08369203, 0.65761839]])\n",
        "\n",
        "y2 = jnp.array([[0.5806555 ], [0.47827308], [0.61024271], [0.15632305], [0.93126525], [0.91945009], [0.1717938 ],\n",
        "                [0.92275104], [0.78164574], [0.71781675]])\n",
        "\n",
        "print(A2.shape)\n",
        "print(y2.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QoKi0-TNgHAy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3820ec4-c615-414f-bee6-4d00eab10545"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "solution for x1:\n",
            "[[ 0.6593367 ]\n",
            " [ 0.5372475 ]\n",
            " [-0.36592606]\n",
            " [-0.74296373]\n",
            " [-0.3301417 ]\n",
            " [-0.01697011]\n",
            " [ 0.02577907]\n",
            " [ 0.9109275 ]]\n",
            "solution for x2:\n",
            "[[ 0.15354455]\n",
            " [ 0.5637012 ]\n",
            " [ 0.1599897 ]\n",
            " [ 0.45920745]\n",
            " [-0.2869888 ]\n",
            " [ 0.18958846]]\n"
          ]
        }
      ],
      "source": [
        "# for part 4.2, write your code here\n",
        "\n",
        "import jax.numpy as jnp\n",
        "from jax import grad\n",
        "\n",
        "\"\"\"\n",
        "Solve the Ax = y system using gradient descent.\n",
        "\n",
        "Parameters:\n",
        "- A: Matrix (jax.numpy array)\n",
        "- y: Target vector (jax.numpy array)\n",
        "- lr: learning rate\n",
        "- num_steps: Number of iterations\n",
        "\n",
        "Returns:\n",
        "- x: the solution vector that minimizes the objective function.\n",
        "in this case, our objective function is ||Ax-y||^2\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# gradient descent function\n",
        "def gradient_descent(A, y, lr=0.01, num_steps=100):\n",
        "  # define the loss function\n",
        "  def loss_function(x):\n",
        "    return jnp.linalg.norm(A @ x - y ) ** 2\n",
        "\n",
        "\n",
        "  # compute the gradient of the loss function\n",
        "  grad_loss = grad(loss_function)\n",
        "\n",
        "  # initialize x with zeros\n",
        "  x = jnp.zeros((A.shape[1], 1))\n",
        "\n",
        "  # gradient descent iterations\n",
        "  for _ in range(num_steps):\n",
        "\n",
        "    x = x - lr * grad_loss(x)\n",
        "\n",
        "  return x\n",
        "\n",
        "\n",
        "# now let's solve case 1:\n",
        "\n",
        "x1_sol = gradient_descent(A1, y1, lr=0.01, num_steps=1000)\n",
        "print(\"solution for x1:\")\n",
        "print(x1_sol)\n",
        "\n",
        "\n",
        "# now let's solve case 2:\n",
        "\n",
        "x2_sol = gradient_descent(A2, y2, lr=0.01, num_steps=1000)\n",
        "print(\"solution for x2:\")\n",
        "print(x2_sol)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}